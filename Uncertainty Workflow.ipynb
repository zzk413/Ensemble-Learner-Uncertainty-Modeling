{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "# Uncertainty in Ensemble Learners\n",
    "\n",
    "#### Zhikai (Ben) Zhong, Undergraduate Student, The University of Texas at Austin\n",
    "\n",
    "##### [LinkedIn](https://www.linkedin.com/in/benzhong)\n",
    "\n",
    "#### Supervised by:\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "#### For PGE 376\n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Loading the required libraries\n",
    "The following code loads the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                   # to set current working directory \n",
    "import math                                                 # basic calculations like square root\n",
    "from sklearn import metrics                                 # measures to check our models\n",
    "from sklearn.tree import export_graphviz                    # graphical visualization of trees\n",
    "from sklearn.preprocessing import StandardScaler            # standardize variables to mean of 0.0 and variance of 1.0\n",
    "\n",
    "from sklearn.model_selection import cross_val_score         # cross validation methods\n",
    "from sklearn.tree import DecisionTreeRegressor              # decision tree method\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingRegressor               # bagging tree method\n",
    "from sklearn.ensemble import RandomForestRegressor          # random forest method\n",
    "from sklearn.ensemble import ExtraTreesRegressor            # extra trees method\n",
    "from sklearn.ensemble import GradientBoostingRegressor      # gradient boosting method\n",
    "from sklearn.ensemble import AdaBoostRegressor              # adaboosting method\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import tree                                    # tree program from scikit learn (package for machine learning)\n",
    "from sklearn.tree import _tree                              # for accessing tree information\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd                                         # DataFrames and plotting\n",
    "import pandas.plotting as pd_plot\n",
    "import numpy as np                                          # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                             # plotting\n",
    "from subprocess import check_call\n",
    "\n",
    "from pylab import *\n",
    "from scipy.stats import norm\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual                      # widgets and interactivity\n",
    "from ipywidgets import widgets                            \n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import Label\n",
    "from ipywidgets import VBox, HBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions\n",
    "There are two functions: \n",
    "1. **create_df (model, X_test, y_test)**\n",
    "\n",
    "\n",
    "2. **plot_df (df, dx)**\n",
    "\n",
    "The **create_df** function creates a pandas dataframe where each row represents a testing sample, first columns is the true value of the testing sample, and every column after represents the prediction of each estimators in a ensemble learner.\n",
    "\n",
    "The **plot_df** function takes in the aforementioned dataframe and returns an accuracy plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=[.1,.2,.3,.4,.5,.6,.7,.8,.9]\n",
    "\n",
    "#a function that creates a dataframe where each column contains an estimator's prediction\n",
    "#with the first column being the true value\n",
    "#and each row corresponds to each sample in the testing set\n",
    "def create_df(Model,X_test,y_test):                         \n",
    "    df=pd.DataFrame(y_test)                                 #creating the dataframe with true value in it\n",
    "    df.columns.values[0] = \"True_Value\"\n",
    "    df=df.reset_index(drop=True)\n",
    "    count=0\n",
    "    for i in Model.estimators_:                             #looping over the model's individual estimators(trees)  \n",
    "        count+=1\n",
    "        Df=pd.DataFrame(i.predict(X_test))\n",
    "        Df.columns=['L='+str(count)]    \n",
    "        df=pd.concat([df, Df], axis=1)                      #add each prediction to a new column \n",
    "    return df\n",
    "\n",
    "#a function that takes in the aforementioned dataframe\n",
    "#and plots an accuracy plot for the ensemble learner\n",
    "#dx is the confidence interval step \n",
    "def plot_df(df,dx):\n",
    "    m,n=df.shape\n",
    "    percentile=np.arange(0,1,dx)\n",
    "    percentage=[]\n",
    "    for i in range(0,m):                                    #looping over each row        \n",
    "        find=False; count=0\n",
    "        True_value=df.iloc[i,0]\n",
    "        row=df.iloc[i,1:]\n",
    "        while find==False and count<len(percentile):\n",
    "            for percent in percentile:                      #looping over each row\n",
    "                down=np.percentile(row,(1-percent)/.02); up=np.percentile(row, 100-(1-percent)/.02)\n",
    "                if True_value<=up and True_value>=down:     #finding the percentile of true value among the estimated values\n",
    "                    percentage=np.append(percentage,percent)\n",
    "                    find=True\n",
    "                    break\n",
    "                else:\n",
    "                    count+=1\n",
    "    lst=[]\n",
    "    for i in percentile:                                    #calculating % samples in each CI\n",
    "        s=sum([1 for x in percentage if x <=i])\n",
    "        lst=np.append(lst,s)\n",
    "    lst/=m  \n",
    "\n",
    "    #the diagonal line\n",
    "    x=np.linspace(0,1,len(lst))\n",
    "    y=x\n",
    "    \n",
    "    #goodness measure\n",
    "    goodness=round(sum((lst-y)*dx),4)\n",
    "    \n",
    "    #plotting accuracy plot\n",
    "    plt.plot(x,y,'black'); plt.text(.5,.15,'Goodness:'+str(goodness),bbox={'facecolor':'grey','alpha':0.2,'pad':10}); \n",
    "    plt.plot(x,np.array(lst),'ro')\n",
    "    plt.xlabel('Probability Interval', fontsize=14); plt.ylabel('Percentage of Samples ', fontsize=14); \n",
    "    plt.title('Accuracy Plot', fontsize=20)\n",
    "    fill([0,1,1,0], [0,1,1,1], 'black', alpha=0.1, edgecolor='black')\n",
    "    \n",
    "    return goodness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping The Data\n",
    "\n",
    "### Loading dataset\n",
    "\n",
    "The dataset is composed of porosity, brittleness, permeability and other positive variables. \n",
    "\n",
    "Therefore we assume all the negative values to be 0 due to inaccuracy in measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WellIndex</th>\n",
       "      <th>Por</th>\n",
       "      <th>LogPerm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Production</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15.91</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.06</td>\n",
       "      <td>14.05</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.85</td>\n",
       "      <td>177.381958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15.34</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.60</td>\n",
       "      <td>31.88</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1479.767778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20.45</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.13</td>\n",
       "      <td>63.67</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.53</td>\n",
       "      <td>4421.221583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11.95</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.90</td>\n",
       "      <td>58.81</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1488.317629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>19.53</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.57</td>\n",
       "      <td>43.75</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.11</td>\n",
       "      <td>5261.094919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>19.47</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.73</td>\n",
       "      <td>54.37</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.12</td>\n",
       "      <td>5497.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12.70</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.70</td>\n",
       "      <td>43.03</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1784.266285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>14.15</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.53</td>\n",
       "      <td>39.43</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1943.639213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>16.18</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.45</td>\n",
       "      <td>45.79</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3176.523611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>20.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.78</td>\n",
       "      <td>28.99</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2522.792318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12.25</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.64</td>\n",
       "      <td>64.08</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1215.676438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>11.59</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.33</td>\n",
       "      <td>59.32</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1364.885428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>18.18</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.85</td>\n",
       "      <td>24.81</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1100.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>12.10</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.10</td>\n",
       "      <td>63.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1166.029442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>14.68</td>\n",
       "      <td>1.47</td>\n",
       "      <td>2.21</td>\n",
       "      <td>46.02</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2717.328863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>9.23</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.85</td>\n",
       "      <td>69.48</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.87</td>\n",
       "      <td>511.608770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>3.83</td>\n",
       "      <td>57.35</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2249.128389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>10.24</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.96</td>\n",
       "      <td>30.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.48</td>\n",
       "      <td>555.848934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15.74</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2.78</td>\n",
       "      <td>40.00</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2630.724009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>13.29</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.04</td>\n",
       "      <td>93.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.94</td>\n",
       "      <td>28.423573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>16.57</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.52</td>\n",
       "      <td>32.16</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1822.997389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>17.05</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.64</td>\n",
       "      <td>47.87</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3907.976463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>15.97</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.88</td>\n",
       "      <td>69.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1350.341147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>16.37</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.67</td>\n",
       "      <td>48.01</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3545.451691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>10.52</td>\n",
       "      <td>1.07</td>\n",
       "      <td>4.31</td>\n",
       "      <td>39.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1092.833803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>10.84</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.64</td>\n",
       "      <td>44.30</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1386.661653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>11.72</td>\n",
       "      <td>1.17</td>\n",
       "      <td>3.87</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1750.080154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>11.54</td>\n",
       "      <td>1.10</td>\n",
       "      <td>4.47</td>\n",
       "      <td>37.78</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1213.281884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>13.92</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.54</td>\n",
       "      <td>55.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2252.847709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>17.02</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>41.88</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3426.720690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>971</td>\n",
       "      <td>17.44</td>\n",
       "      <td>1.74</td>\n",
       "      <td>3.44</td>\n",
       "      <td>57.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.38</td>\n",
       "      <td>3742.090380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>972</td>\n",
       "      <td>10.95</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.67</td>\n",
       "      <td>47.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1466.627778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>973</td>\n",
       "      <td>12.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.88</td>\n",
       "      <td>38.29</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1581.271103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>974</td>\n",
       "      <td>15.72</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.51</td>\n",
       "      <td>35.09</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1982.058606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>975</td>\n",
       "      <td>14.02</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.01</td>\n",
       "      <td>62.59</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1757.675482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>976</td>\n",
       "      <td>14.49</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.00</td>\n",
       "      <td>22.54</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.99</td>\n",
       "      <td>484.517529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>977</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.64</td>\n",
       "      <td>35.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1059.672113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>978</td>\n",
       "      <td>19.29</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.82</td>\n",
       "      <td>58.56</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.97</td>\n",
       "      <td>4609.512101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>979</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>3.60</td>\n",
       "      <td>63.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>579.624370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>980</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1.10</td>\n",
       "      <td>3.41</td>\n",
       "      <td>46.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2907.919597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>981</td>\n",
       "      <td>14.48</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.56</td>\n",
       "      <td>47.44</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2576.102155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>982</td>\n",
       "      <td>9.88</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.64</td>\n",
       "      <td>55.19</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1248.129938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>983</td>\n",
       "      <td>18.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.23</td>\n",
       "      <td>74.07</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1401.211709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>984</td>\n",
       "      <td>18.07</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.89</td>\n",
       "      <td>28.67</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1640.769905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>985</td>\n",
       "      <td>15.68</td>\n",
       "      <td>1.68</td>\n",
       "      <td>3.77</td>\n",
       "      <td>57.85</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2733.704192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>986</td>\n",
       "      <td>15.28</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.29</td>\n",
       "      <td>34.78</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1798.975721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>987</td>\n",
       "      <td>10.88</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.09</td>\n",
       "      <td>55.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1482.876885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>988</td>\n",
       "      <td>16.14</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.16</td>\n",
       "      <td>38.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2460.098447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>989</td>\n",
       "      <td>16.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.42</td>\n",
       "      <td>35.84</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2219.562750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>990</td>\n",
       "      <td>13.52</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.45</td>\n",
       "      <td>31.57</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1113.605137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>991</td>\n",
       "      <td>6.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.13</td>\n",
       "      <td>56.69</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.10</td>\n",
       "      <td>714.453061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>992</td>\n",
       "      <td>20.37</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.37</td>\n",
       "      <td>37.76</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4612.097972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>993</td>\n",
       "      <td>12.92</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.68</td>\n",
       "      <td>71.40</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.91</td>\n",
       "      <td>705.663553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>994</td>\n",
       "      <td>17.13</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.88</td>\n",
       "      <td>51.45</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4165.407602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>995</td>\n",
       "      <td>11.95</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.97</td>\n",
       "      <td>67.18</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.06</td>\n",
       "      <td>931.610877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>17.99</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.38</td>\n",
       "      <td>44.32</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.08</td>\n",
       "      <td>4211.527806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.52</td>\n",
       "      <td>57.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1560.333735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>15.55</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.48</td>\n",
       "      <td>58.25</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2858.180459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>20.89</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.23</td>\n",
       "      <td>46.17</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.27</td>\n",
       "      <td>6934.576348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>15.74</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.37</td>\n",
       "      <td>73.08</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.06</td>\n",
       "      <td>963.675793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     WellIndex    Por  LogPerm    AI  Brittle   TOC    VR   Production\n",
       "0            1  15.91     1.67  3.06    14.05  1.36  1.85   177.381958\n",
       "1            2  15.34     1.65  2.60    31.88  1.37  1.79  1479.767778\n",
       "2            3  20.45     2.02  3.13    63.67  1.79  2.53  4421.221583\n",
       "3            4  11.95     1.14  3.90    58.81  0.40  2.03  1488.317629\n",
       "4            5  19.53     1.83  2.57    43.75  1.40  2.11  5261.094919\n",
       "5            6  19.47     2.04  2.73    54.37  1.42  2.12  5497.005506\n",
       "6            7  12.70     1.30  3.70    43.03  0.45  1.95  1784.266285\n",
       "7            8  14.15     1.23  2.53    39.43  0.79  1.54  1943.639213\n",
       "8            9  16.18     1.34  3.45    45.79  0.56  1.95  3176.523611\n",
       "9           10  20.51     2.48  2.78    28.99  1.78  2.24  2522.792318\n",
       "10          11  12.25     1.20  3.64    64.08  0.81  2.44  1215.676438\n",
       "11          12  11.59     1.06  3.33    59.32  0.24  1.91  1364.885428\n",
       "12          13  18.18     2.02  2.85    24.81  1.17  1.84  1100.000817\n",
       "13          14  12.10     1.42  4.10    63.71  0.09  2.12  1166.029442\n",
       "14          15  14.68     1.47  2.21    46.02  1.27  1.67  2717.328863\n",
       "15          16   9.23     0.81  2.85    69.48  0.69  1.87   511.608770\n",
       "16          17  14.00     1.65  3.83    57.35  1.12  2.32  2249.128389\n",
       "17          18  10.24     0.88  2.96    30.03  0.43  1.48   555.848934\n",
       "18          19  15.74     1.20  2.78    40.00  1.49  2.25  2630.724009\n",
       "19          20  13.29     1.15  3.04    93.47  0.30  1.94    28.423573\n",
       "20          21  16.57     1.85  2.52    32.16  1.43  1.78  1822.997389\n",
       "21          22  17.05     1.69  3.64    47.87  1.17  2.34  3907.976463\n",
       "22          23  15.97     1.37  2.88    69.75  0.90  1.81  1350.341147\n",
       "23          24  16.37     1.34  2.67    48.01  1.28  2.16  3545.451691\n",
       "24          25  10.52     1.07  4.31    39.33  0.13  2.18  1092.833803\n",
       "25          26  10.84     0.58  2.64    44.30  0.41  1.41  1386.661653\n",
       "26          27  11.72     1.17  3.87    51.55  0.71  2.39  1750.080154\n",
       "27          28  11.54     1.10  4.47    37.78  0.67  2.51  1213.281884\n",
       "28          29  13.92     1.45  3.54    55.46  0.62  1.89  2252.847709\n",
       "29          30  17.02     1.92  3.08    41.88  1.26  2.06  3426.720690\n",
       "..         ...    ...      ...   ...      ...   ...   ...          ...\n",
       "970        971  17.44     1.74  3.44    57.33  1.33  2.38  3742.090380\n",
       "971        972  10.95     0.80  3.67    47.06  0.12  1.93  1466.627778\n",
       "972        973  12.94     0.97  2.88    38.29  1.25  1.94  1581.271103\n",
       "973        974  15.72     1.33  2.51    35.09  1.42  1.95  1982.058606\n",
       "974        975  14.02     1.50  3.01    62.59  1.04  2.16  1757.675482\n",
       "975        976  14.49     1.58  3.00    22.54  1.08  1.99   484.517529\n",
       "976        977  11.80     1.07  3.64    35.35  0.35  1.76  1059.672113\n",
       "977        978  19.29     1.61  2.82    58.56  1.23  1.97  4609.512101\n",
       "978        979   7.22     0.35  3.60    63.09  0.00  1.67   579.624370\n",
       "979        980  15.70     1.10  3.41    46.86  0.14  1.93  2907.919597\n",
       "980        981  14.48     1.34  3.56    47.44  0.67  2.11  2576.102155\n",
       "981        982   9.88     1.00  3.64    55.19  0.52  2.16  1248.129938\n",
       "982        983  18.80     1.80  3.23    74.07  1.66  2.54  1401.211709\n",
       "983        984  18.07     1.33  2.89    28.67  1.52  2.10  1640.769905\n",
       "984        985  15.68     1.68  3.77    57.85  0.72  2.35  2733.704192\n",
       "985        986  15.28     1.25  2.29    34.78  1.28  1.61  1798.975721\n",
       "986        987  10.88     1.20  4.09    55.09  0.81  2.40  1482.876885\n",
       "987        988  16.14     1.62  3.16    38.03  0.97  2.05  2460.098447\n",
       "988        989  16.12     1.41  2.42    35.84  1.48  1.84  2219.562750\n",
       "989        990  13.52     1.35  3.45    31.57  1.51  2.22  1113.605137\n",
       "990        991   6.55     0.12  4.13    56.69  0.28  2.10   714.453061\n",
       "991        992  20.37     1.90  2.37    37.76  1.40  1.99  4612.097972\n",
       "992        993  12.92     1.36  3.68    71.40  0.11  1.91   705.663553\n",
       "993        994  17.13     1.79  2.88    51.45  1.77  2.28  4165.407602\n",
       "994        995  11.95     1.14  2.97    67.18  0.80  2.06   931.610877\n",
       "995        996  17.99     2.29  3.38    44.32  0.98  2.08  4211.527806\n",
       "996        997  12.12     0.82  3.52    57.07  0.00  1.73  1560.333735\n",
       "997        998  15.55     1.50  2.48    58.25  1.89  2.35  2858.180459\n",
       "998        999  20.89     2.02  3.23    46.17  1.71  2.27  6934.576348\n",
       "999       1000  15.74     1.31  2.37    73.08  1.24  2.06   963.675793\n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"unconv_MV.csv\") \n",
    "num = my_data._get_numeric_data() # get the numerical values\n",
    "num[num < 0] = 0  #filtering out unrealistic values that are less than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "We split the data into training and testing data for the purpose of cross validation.\n",
    "\n",
    "We use the command train_test_split from sklearn. \n",
    "\n",
    "We set the **random_state** variable to 0 (or any other integer) to make sure our results are reproducible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Por',\n",
    "                'Brittle',\n",
    "                'LogPerm',\n",
    "                'AI',\n",
    "                'TOC',\n",
    "                'VR'\n",
    "               ]\n",
    "X=my_data[feature_cols]\n",
    "y=my_data.Production\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0) #50% Testing 50% Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking At The Model\n",
    "\n",
    "### Building The Model\n",
    "\n",
    "In this example, we use randomforest regressor to predict the intial gas production with several features.\n",
    "\n",
    "We are going to create a model and then cross validate it with the testing data and give it a regression score.\n",
    "\n",
    "The best score is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9436744501278863"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model=RandomForestRegressor(n_estimators=300, random_state=0,oob_score=True)\n",
    "original_model=original_model.fit(X_train,y_train)\n",
    "original_df=create_df(original_model,X_test,y_test)\n",
    "explained_variance_score(y_test, original_model.predict(X_test)) \n",
    "#See the correlation between our model and the testing data\n",
    "#We have a pretty good score, meaning our model does a good job predicitng gas production\n",
    "#However, does our model have an accurate uncertainty model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Estimators\n",
    "\n",
    "Ensemble learners are consisted of many individual estimators (trees). Each individual estimators will make a prediction and we yield the final prediction from the results of those estimators predictions.\n",
    "\n",
    "We can have each individual tree make a prediction and plot their distribution in a histogram.\n",
    "\n",
    "Ideally, this should be a normal distribution. \n",
    "\n",
    "We need to be able to offer a confidence interval for our prediction.\n",
    "\n",
    "Ideally, X% of the testing samples should fall into the confidence interval of X%.\n",
    "* For instance, 95% of times, the testing sample should fall into the model's 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction Distribution')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=original_df.iloc[0,1:]                                 # For example, let's pick the 1st testing sample to graph\n",
    "height, bins, patches=plt.hist(preds,alpha=.15,bins=20,color='r')\n",
    "ci = norm(*norm.fit(preds)).interval(0.8)               # Confidence interval of 90%\n",
    "plt.fill_betweenx([0, height.max()], ci[0], ci[1], color='g', alpha=0.2) \n",
    "plt.xlabel('Production Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Distribution', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Modeling\n",
    "\n",
    "### Plotting Accuracy Plot\n",
    "\n",
    "Use the previously defined functions to determine whether the model is overfitting or underfitting.\n",
    "\n",
    "We are going to plot the proportion of true data within [10,20,30,40,50,60,70,80,90]% interval. \n",
    "\n",
    "If the datapoints land in the shaded area, that means the model is **accurate but imprecise**.\n",
    "\n",
    "If the datapoints land on the line, that means the model is **accurate and precise**.\n",
    "\n",
    "If the datapoints land in the unshaded area, that means the model is **inaccurate and imprecise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0814"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(6,6))\n",
    "plot_df(original_df,.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving The Uncertainty Model\n",
    "The hypothesis is that we can improve the precision of our model by simplifying the each individual estimators of an ensembler learner. \n",
    "\n",
    "This could be done in many ways simply by adjusting the hyperparameters of an ensemble learner.\n",
    "\n",
    "Here are two common hyperparameters we can adjust:\n",
    "\n",
    "**1. min_samples_leaf**\n",
    "\n",
    "This controls the minimum sample of a leaf/leaves. When splitting, only nodes that contains more samples than the threshold would be split into a leaf.\n",
    "\n",
    "**2. max_features**\n",
    "\n",
    "This controls the number of features to be considered when splitting. A tree would only split/grow by considering the amount of variables at the threshold of this variable.\n",
    "\n",
    "**3. max_depth**\n",
    "\n",
    "This controls the maximum depth an individual estimator (tree) will grow. Nodes will stop splitting once the tree depth has grown to this value.\n",
    "\n",
    "**4. max_leaf_nodes**\n",
    "\n",
    "This controls the maximum leaf nodes when trees are grown. The best nodes are kept, in terms of relative reduction in impurity.\n",
    "#### Hyperparameter 1, 3, 4 affects the size of each estimator (tree), whereas 2 does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Feature\n",
    "There are 6 features in this dataset. We are going to adjust the maximum number of features when splitting trees to see how that is going to affect the uncertainty model.\n",
    "\n",
    "When restricting the max features, all the trees are still fully grown. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1a54c14694435487d2badf40fa89ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-77fd8d5e796c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplot_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max Feature='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0d35eab68d74>\u001b[0m in \u001b[0;36mplot_df\u001b[0;34m(df, dx)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpercent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m:\u001b[0m                      \u001b[0;31m#looping over each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mdown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mTrue_value\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mup\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mTrue_value\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;31m#finding the percentile of true value among the estimated values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mpercentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3705\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Percentiles must be in the range [0, 100]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3706\u001b[0m     return _quantile_unchecked(\n\u001b[0;32m-> 3707\u001b[0;31m         a, q, axis, out, overwrite_input, interpolation, keepdims)\n\u001b[0m\u001b[1;32m   3708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, interpolation, keepdims)\u001b[0m\n\u001b[1;32m   3824\u001b[0m     r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,\n\u001b[1;32m   3825\u001b[0m                     \u001b[0moverwrite_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3826\u001b[0;31m                     interpolation=interpolation)\n\u001b[0m\u001b[1;32m   3827\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3379\u001b[0m     \"\"\"\n\u001b[0;32m-> 3380\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3381\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    714\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    715\u001b[0m         \"\"\"\n\u001b[0;32m--> 716\u001b[0;31m         if (dtype is None and isinstance(self.array, ABCDatetimeArray)\n\u001b[0m\u001b[1;32m    717\u001b[0m                 and getattr(self.dtype, 'tz', None)):\n\u001b[1;32m    718\u001b[0m             msg = (\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPandasArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPandasArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_features=[1,2,3,4,5,6]\n",
    "for i in tqdm(max_features):\n",
    "    plt.subplot(2, 3, i)\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=0, \n",
    "                                #random_state sets the random seed to ensure we get the same result everytime\n",
    "                                min_samples_leaf=1, oob_score=True,\n",
    "                               max_features=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Max Feature='+str(i),fontsize=14)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "#The results show that max features don't have a lot of effect on improving the uncertainty model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Sample Leaf\n",
    "\n",
    "Now we are going to explore how minimum leaves are going to affect uncertainty modeling. \n",
    "\n",
    "This **min_samples_leaf** restrict the minimum number of leaves on a leaf node. Therefore all the trees will be pruned to a certain degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_leaf=[1,6,12,18,22,25]\n",
    "index=1\n",
    "for i in tqdm(min_leaf):\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=0, \n",
    "                                min_samples_leaf=i, oob_score=True,max_features='auto'\n",
    "                               )\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Min Leaf='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "#As you can see from the graphs below, tree complexity affects the uncertainty model\n",
    "#When min leaf is 1, the model is underfitting\n",
    "#When min leaf gets above 20, the model starts to overfit\n",
    "#The most optimal setting would probably be 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Depth\n",
    "\n",
    "The default value is None. When it's none, all the trees will be fully grown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_depth=[12,6,5,4,3,2]\n",
    "index=1\n",
    "for i in tqdm(max_depth):\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=0,\n",
    "                              max_depth=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Max Depth='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "\n",
    "#As you can see from the graphs below, tree complexity affects the uncertainty model\n",
    "#When max_depth is 2, the model is inaccurate and imprecise\n",
    "#When max_depth gets above 5, the model starts to underfit\n",
    "#The most optimal setting would probably be 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Leaf Nodes\n",
    "\n",
    "The default value is None. When it's none, all the trees are fully grown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes=[80,40,25,15,10,8]\n",
    "index=1\n",
    "for i in tqdm(max_nodes):\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=42,\n",
    "                              max_leaf_nodes=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Max Nodes='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "\n",
    "#As you can see from the graphs below, tree complexity affects the uncertainty model\n",
    "#When max_leaf_nodes is 5, the model is inaccurate and imprecise\n",
    "#When max_leaf_nodes gets above 10, the model starts to underfit\n",
    "#The most optimal setting would probably be 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes=np.arange(5,100,)\n",
    "goodness=[]\n",
    "for i in tqdm(max_nodes):\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=42, max_leaf_nodes=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    goodness=np.append(goodness,plot_df(df,.02))\n",
    "\n",
    "plt.plot(max_nodes,goodness)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**Size of the trees affects uncertainty modeling whereas restraining splitting criterias does not seem to help with uncertainty modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Model Performance\n",
    "\n",
    "Since we found out that min_samples_leaf's optimal value is 20. We can build another model with this hyperparameter setting and see how well the model can perform.\n",
    "\n",
    "Because  we increased the minimum samples per leaf, we decreased the complexity of the model and this trade-off could result in a poorer fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "improved_model=RandomForestRegressor(n_estimators=100, random_state=0, max_leaf_nodes=30,oob_score=True)\n",
    "improved_model=improved_model.fit(X_train,y_train)\n",
    "improved_df=create_df(improved_model,X_test,y_test)\n",
    "\n",
    "original_model=RandomForestRegressor(n_estimators=100, random_state=0, min_samples_leaf=1,oob_score=True)\n",
    "original_model=original_model.fit(X_train,y_train)\n",
    "original_df=create_df(original_model,X_test,y_test)\n",
    "\n",
    "print('Model score original', explained_variance_score(y_test, original_model.predict(X_test)))\n",
    "print('Model score adjusted', explained_variance_score(y_test, improved_model.predict(X_test)))\n",
    "\n",
    "#When simplifying our model, we sacrificed some of our accuracy as seen by the decrease in the correlation score\n",
    "#0.8 is still an acceptable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plot_df(improved_df,.02)\n",
    "#Now our uncertainty model is more or less on the line \n",
    "#We improved the precision of our model by sacrificing some of our accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change in Prediction Distribution \n",
    "\n",
    "Now let's look at the prediction distribution again.\n",
    "\n",
    "Adjusted predcition distribution is a lot more normal than the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(n,confidence): \n",
    "    #plt.clf()\n",
    "    fig, (ax1,ax2)=plt.subplots(1,2)\n",
    "    \n",
    "    preds=original_df.iloc[n,1:]; improved_pred=improved_df.iloc[n,1:]\n",
    "    preds_true=float(original_df.iloc[n,0]); improved_true=float(improved_df.iloc[n,0])\n",
    "    # For example, let's pick the nth testing sample to graph\n",
    "    \n",
    "    height, bins, patches=ax1.hist(preds, alpha=.25, bins=15, color='r'); height2, bins2, patches2=ax2.hist(improved_pred, alpha=.25, bins=15, color='r')\n",
    "    ax1.axvline(preds_true, color='k', linewidth=1,label='True Value'); ax2.axvline(improved_true, color='k', linewidth=1,label='True Value')\n",
    "    \n",
    "    ylim1=ax1.get_ylim(); xlim1=ax1.get_xlim(); xlim2=ax2.get_xlim(); ylim2=ax2.get_ylim()\n",
    "    xlim=[min(np.append(xlim1,xlim2)),max(np.append(xlim1,xlim2))]; ylim=[min(np.append(ylim1,ylim2)),max(np.append(ylim1,ylim2))]\n",
    "    \n",
    "    #normal CI\n",
    "    #ci1 = norm(*norm.fit(preds)).interval(confidence)   \n",
    "    #ci2 = norm(*norm.fit(improved_pred)).interval(confidence) \n",
    "    \n",
    "    #quantile CI\n",
    "    ci1=[np.percentile(preds,(1-confidence)*50), np.percentile(preds, 100-(1-confidence)*50)]\n",
    "    ci2=[np.percentile(improved_pred,(1-confidence)*50), np.percentile(improved_pred, 100-(1-confidence)*50)]\n",
    "    \n",
    "    ax1.fill_betweenx([0, height.max()], ci1[0], ci1[1], color='g', alpha=0.2)  \n",
    "   \n",
    "               \n",
    "    ax2.fill_betweenx([0, height2.max()], ci2[0], ci2[1], color='g', alpha=0.2)\n",
    "    \n",
    "    ax2.set(title='Adjusted',xlabel='Production Prediction',ylabel='Frequency')\n",
    "    ax2.set_xlim(xlim); ax2.set_ylim(ylim)\n",
    "    ax1.set(title='Original',xlabel='Production Prediction',ylabel='Frequency')\n",
    "    ax1.set_xlim(xlim); ax1.set_ylim(ylim)\n",
    "    \n",
    "    fig.set_figheight(8); fig.set_figwidth(15)\n",
    "    \n",
    "    ax1.legend();ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = widgets.Text(value='Estimators distribution before and after adjustment',layout=Layout(width='950px', height='30px'))\n",
    "\n",
    "#index number of the sample we are plotting\n",
    "n = widgets.IntSlider(min=0, max = len(improved_df)-1, value = 0, step = 1, description = '$N$th sample',orientation='horizontal',continuous_update=False, layout=Layout(width='400px', height='20px'))\n",
    "n.style.handle_color = 'gray'\n",
    "\n",
    "#confidence interval we are plotting\n",
    "confidence = widgets.FloatSlider(min=0.05, max = .96, value = 0.7, step = .05, description = '$Confidence$',orientation='horizontal', continuous_update=False, layout=Layout(width='400px', height='20px'))\n",
    "confidence.style.handle_color = 'green'\n",
    "\n",
    "ui = widgets.HBox([n,confidence],) \n",
    "ui2 = widgets.VBox([l,ui],)\n",
    "\n",
    "interactive_plot = widgets.interactive_output(compare, {'n':n, 'confidence': confidence})\n",
    "interactive_plot.clear_output(wait = True)\n",
    "\n",
    "display(ui2, interactive_plot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Modeling With Other Ensemble Methods\n",
    "\n",
    "### Extra Trees \n",
    "\n",
    "After focusing on Random Forest Regressor, we can take a look at Extra Trees Regressor, which is very similar to Random Forest.\n",
    "\n",
    "#### Minimum Sample Leaf\n",
    "We are going to check how min_leaf affects the uncertainty model of ExtraTreesRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_leaf=[1,3,6,7,8,10]\n",
    "index=1\n",
    "for i in min_leaf:\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=ExtraTreesRegressor(n_estimators=300, random_state=0, \n",
    "                                min_samples_leaf=i, max_features='auto',\n",
    "                              bootstrap=True,oob_score=True\n",
    "                               )\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,percentile)\n",
    "    plt.title('Min Leaf='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "#As we can see ExtraTreesRegressor is a little different from RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Depth\n",
    "Now check how max_depth affects the uncertainty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_depth=[12,6,5,4,3,2]\n",
    "index=1\n",
    "for i in max_depth:\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=ExtraTreesRegressor(n_estimators=300, random_state=0,\n",
    "                                max_depth=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Max Depth='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "\n",
    "#As you can see from the graphs below, tree complexity affects the uncertainty model\n",
    "#When max_depth is 2, the model is inaccurate and imprecise\n",
    "#When max_depth gets above 5, the model starts to underfit\n",
    "#The most optimal setting would probably be 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Leaf Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes=[80,30,20,15,14,13]\n",
    "index=1\n",
    "for i in tqdm(max_nodes):\n",
    "    plt.subplot(2, 3, index)\n",
    "    model=ExtraTreesRegressor(n_estimators=300, random_state=42,\n",
    "                              max_leaf_nodes=i)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,.02)\n",
    "    plt.title('Max Nodes='+str(i),fontsize=14)\n",
    "    index+=1\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "\n",
    "#As you can see from the graphs below, tree complexity affects the uncertainty model\n",
    "#When max_leaf_nodes is 5, the model is inaccurate and imprecise\n",
    "#When max_leaf_nodes gets above 10, the model starts to underfit\n",
    "#The most optimal setting would probably be 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model=ExtraTreesRegressor(n_estimators=300, random_state=0, min_samples_leaf=7)\n",
    "improved_model=improved_model.fit(X_train,y_train)\n",
    "improved_df=create_df(improved_model,X_test,y_test)\n",
    "\n",
    "original_model=ExtraTreesRegressor(n_estimators=300, random_state=0, min_samples_leaf=1)\n",
    "original_model=original_model.fit(X_train,y_train)\n",
    "original_df=create_df(original_model,X_test,y_test)\n",
    "print('R2 before', explained_variance_score(y_test, original_model.predict(X_test)))\n",
    "print('R2 after', explained_variance_score(y_test, improved_model.predict(X_test)))\n",
    "#When simplifying our model, we sacrificed some of our accuracy as seen by the decrease in the correlation score\n",
    "#0.8 is still an acceptable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = widgets.Text(value='Estimators distribution before and after adjustment',layout=Layout(width='950px', height='30px'))\n",
    "\n",
    "#index number of the sample we are plotting\n",
    "n = widgets.IntSlider(min=0, max = len(improved_df)-1, value = 0, step = 1, description = '$N$th sample',orientation='horizontal',continuous_update=False, layout=Layout(width='400px', height='20px'))\n",
    "n.style.handle_color = 'gray'\n",
    "\n",
    "#confidence interval we are plotting\n",
    "confidence = widgets.FloatSlider(min=0.05, max = .96, value = 0.7, step = .05, description = '$Confidence$',orientation='horizontal', continuous_update=False, layout=Layout(width='400px', height='20px'))\n",
    "confidence.style.handle_color = 'green'\n",
    "\n",
    "ui = widgets.HBox([n,confidence],) \n",
    "ui2 = widgets.VBox([l,ui],)\n",
    "\n",
    "interactive_plot = widgets.interactive_output(compare, {'n':n, 'confidence': confidence})\n",
    "interactive_plot.clear_output(wait = True)\n",
    "\n",
    "display(ui2, interactive_plot)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Methods\n",
    "\n",
    "#### Plotting Accuracy Plot For Gradient Boost\n",
    "\n",
    "Boosting has a different mechanism where the individual estimators are predicting loss, we will need to define a new function to create the dataframe we have been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GB_plot(X_train,y_train,X_test,y_test,percentile,\n",
    "           min_samples_leaf=1,max_depth=3,min_samples_split=2,n_estimators=200):\n",
    "    df=pd.DataFrame(y_test)\n",
    "    df=df.rename(columns={'Production':'True_Value'})\n",
    "    df=df.reset_index(drop=True)\n",
    "    # Set lower and upper quantile\n",
    "    # Each model has to be separate\n",
    "    m,n=df.shape\n",
    "    percentage=[]\n",
    "    for percent in percentile:\n",
    "        UPPER_ALPHA = (1-(percent/100))/2+(percent/100)\n",
    "        LOWER_ALPHA = 1-UPPER_ALPHA\n",
    "        lower= GradientBoostingRegressor(loss=\"quantile\",                   \n",
    "                                         alpha=LOWER_ALPHA,\n",
    "                                         random_state=0,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         n_estimators=n_estimators)\n",
    "        upper = GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                          alpha=UPPER_ALPHA,\n",
    "                                          random_state=0,\n",
    "                                          min_samples_leaf=min_samples_leaf,\n",
    "                                          max_depth=max_depth,\n",
    "                                          min_samples_split=min_samples_split,\n",
    "                                          n_estimators=n_estimators)\n",
    "        lower.fit(X_train,y_train)\n",
    "        upper.fit(X_train,y_train)\n",
    "        count=0\n",
    "        for i in range(0,m):\n",
    "            True_value=df.iloc[i,0]\n",
    "            #CI upper and lower bound is calculated here\n",
    "            down=lower.predict(pd.DataFrame(X_test.iloc[i]).transpose())\n",
    "            up=upper.predict(pd.DataFrame(X_test.iloc[i]).transpose())\n",
    "\n",
    "            #determining if the true value falls within the CI\n",
    "            if True_value<up and True_value>down:\n",
    "                count+=1\n",
    "        percentage=np.append(percentage,count/m)\n",
    "    x = np.linspace(0,1,5)\n",
    "    y = x\n",
    "    plt.plot(x, y, 'black')\n",
    "    plt.plot(np.array(percentile)/100,np.array(percentage),'ro')\n",
    "    plt.xlabel('Probability Interval', fontsize=14)\n",
    "    plt.ylabel('Percentage of Samples ', fontsize=14)\n",
    "    plt.title('Accuracy Plot', fontsize=20)\n",
    "    fill([0,1,1,0], [0,1,1,1], 'black', alpha=0.1, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "GB_plot(X_train,y_train,X_test,y_test,percentile,\n",
    "       min_samples_leaf=10,max_depth=4,min_samples_split=10,\n",
    "       n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=GradientBoostingRegressor(n_estimators=300, random_state=0)\n",
    "model2=model2.fit(X_train,y_train)\n",
    "\n",
    "print('R2', explained_variance_score(y_test, model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Size And Uncertainty Modeling\n",
    "\n",
    "The size of the dataset we used was 1000 rows. \n",
    "\n",
    "Real life dataset would be of many different sizes.\n",
    "\n",
    "Because the our trianing set sizes could be different, our parameters for uncertainty modeling should change too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Test Size Could Affect Uncertainty Modeling\n",
    "\n",
    "Remember before creating the regression model, we had to split our dataset into training and testing datasets.\n",
    "\n",
    "Because the ratio of testing and training sets can different depending on how one sets it, it could potentially affect our uncertainty model.\n",
    "\n",
    "Here we are going to examine how different test size from .1 to .9 of the whole dataset could affect our uncertainty model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Por',\n",
    "                'Brittle',\n",
    "                'LogPerm',\n",
    "                'AI',\n",
    "                'TOC',\n",
    "                'VR'\n",
    "               ]\n",
    "\n",
    "for i in [.5,.6,.7,.8,.9,1]:\n",
    "    X=my_data[feature_cols].sample(int(len(my_data)*.8))\n",
    "    y=my_data.Production[X.index]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "    \n",
    "    #50% Testing 50% Training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i,dx):\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=i, random_state=0)\n",
    "    model=RandomForestRegressor(n_estimators=300, random_state=0, min_samples_leaf=1)\n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    m,n=df.shape\n",
    "    percentile=np.arange(0,1,dx)\n",
    "    percentage=[]\n",
    "    for i in range(0,m):\n",
    "        find=False\n",
    "        count=0\n",
    "        True_value=df.iloc[i,0]\n",
    "        row=df.iloc[i,1:]\n",
    "        while find==False and count<len(percentile):\n",
    "            for percent in percentile: \n",
    "                down=np.percentile(row,(1-percent)/.02); up=np.percentile(row, 100-(1-percent)/.02)\n",
    "                if True_value<=up and True_value>=down:\n",
    "                    percentage=np.append(percentage,percent)\n",
    "                    find=True\n",
    "                    break\n",
    "                else:\n",
    "                    count+=1\n",
    "    lst=[]\n",
    "    for i in percentile:\n",
    "        s=sum([1 for x in percentage if x <=i])\n",
    "        lst=np.append(lst,s)\n",
    "    lst/=m                 \n",
    "    return lst\n",
    "\n",
    "def update(i=0):\n",
    "    plt.figure(num=None, figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    x = np.linspace(0,1,5)\n",
    "    y = x\n",
    "    #plotting accuracy plot\n",
    "    plt.plot(x, y, 'black')\n",
    "    plt.plot(np.array(percentile)/100,lst[int(i*10)-1],'ro')\n",
    "    plt.xlabel('Probability Interval', fontsize=14)\n",
    "    plt.ylabel('Percentage of Samples ', fontsize=14)\n",
    "    plt.title('Accuracy Plot', fontsize=20)\n",
    "    fill([0,1,1,0], [0,1,1,1], 'black', alpha=0.1, edgecolor='black')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[test(i,.02) for i in tqdm(test_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interact(update, i = widgets.FloatSlider(value=.1,\n",
    "                                               min=.1,\n",
    "                                               max=.9,\n",
    "                                               step=.1,\n",
    "                                        description='Test Size',\n",
    "                                        continuous_update=True))\n",
    "#According to the graph below, the model is underfitting because the data points all landed in the shaded zone\n",
    "#For example, looking at test size=.5, at 50% CI, more than 60% of the testing samples fell into their corresponding CI\n",
    "#This means that the model is underfitting: the CI is too wide to be precise.\n",
    "#For consistency, we are going to set our test size to .5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the histogram\n",
    "values, base = np.histogram(data, bins=90)\n",
    "values2, base2 = np.histogram(data2,bins=90)\n",
    "#evaluate the cumulative\n",
    "cumulative = np.cumsum(values)\n",
    "cumulative2 = np.cumsum(values2)\n",
    "# plot the cumulative function\n",
    "plt.plot(base[:-1], cumulative, c='r')\n",
    "plt.plot(base2[:-1],cumulative2, c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, ratio, random_state=0):\n",
    "    X_train, X_test, \n",
    "    y_train, y_test =train_test_split(X, y, test_size=ratio, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=[1,2,3,4,5,6]\n",
    "for i in max_features:\n",
    "    plt.subplot(2, 3, i)\n",
    "    model=ExtraTreesRegressor(n_estimators=200, random_state=0, \n",
    "                                min_samples_leaf=1,\n",
    "                               max_features=i)\n",
    "    \n",
    "    model=model.fit(X_train,y_train)\n",
    "    df=create_df(model,X_test,y_test)\n",
    "    plot_df(df,percentile)\n",
    "    plt.title('Max Feature='+str(i),fontsize=14)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3)\n",
    "#The results show that max features don't have a lot of effect on improving the uncertainty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''scrap code \n",
    "\n",
    "def plot_df(df,percentile):\n",
    "    m,n=df.shape\n",
    "    percentage=[]\n",
    "    for percent in percentile:\n",
    "        count=0\n",
    "        for i in range(0,m):\n",
    "            True_value=df.iloc[i,0]\n",
    "            a=df.iloc[i,1:]\n",
    "            \n",
    "            #normal CI is calculated here\n",
    "            #ci1 = norm(*norm.fit(a)).interval(percent/100)\n",
    "            #down=ci1[0]\n",
    "            #up=ci1[1]\n",
    "            \n",
    "            #quantile CI is calculated here\n",
    "            down=np.percentile(a,(100-percent)/2)\n",
    "            up=np.percentile(a, 100-(100-percent)/2)\n",
    "            \n",
    "            #determining if the true value falls within the CI\n",
    "            if True_value<up and True_value>down:\n",
    "                count+=1\n",
    "                \n",
    "        percentage=np.append(percentage,count/m)\n",
    "        \n",
    "    x = np.linspace(0,1,len(percentage))\n",
    "    y = x\n",
    "    score=round(r2_score(x, np.array(percentage)),3)\n",
    "    #plotting accuracy plot\n",
    "    plt.plot(x, y, 'black')\n",
    "    plt.text(.1, .85, '$R^{2}$='+str(score), bbox={'facecolor': 'grey', 'alpha': 0.2, 'pad': 10})\n",
    "    plt.plot(np.array(percentile)/100,np.array(percentage),'ro')\n",
    "    plt.xlabel('Probability Interval', fontsize=14)\n",
    "    plt.ylabel('Percentage of Samples ', fontsize=14)\n",
    "    plt.title('Accuracy Plot', fontsize=20)\n",
    "    fill([0,1,1,0], [0,1,1,1], 'black', alpha=0.1, edgecolor='black')\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
